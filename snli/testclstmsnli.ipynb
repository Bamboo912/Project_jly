{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x215a20b5d08>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "import data_helper\n",
    "\n",
    "# Show warnings and errors only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# File paths\n",
    "\n",
    "tf.flags.DEFINE_string('test_data_file', 'E:\\\\Lessons\\\\MachineLearning\\\\Project\\\\sst2\\\\data\\\\test_data.csv', 'Test data file path')\n",
    "tf.flags.DEFINE_string('run_dir', 'E:\\\\Lessons\\\\MachineLearning\\\\Project\\\\sst2\\\\runs\\\\1619072213', 'Restore the model from this run')\n",
    "tf.flags.DEFINE_string('checkpoint', 'clf-90000', 'Restore the graph from this checkpoint')\n",
    "\n",
    "tf.flags.DEFINE_integer('batch_size', 32, 'Test batch size')\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "# Restore parameters\n",
    "with open(os.path.join(FLAGS.run_dir, 'params.pkl'), 'rb') as f:\n",
    "    params = pkl.load(f, encoding='bytes')\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(os.path.join(FLAGS.run_dir, 'vocab'))\n",
    "data, labels, lengths, _ = data_helper.load_data(file_path=FLAGS.test_data_file,\n",
    "                                                 sw_path=params['stop_word_file'],\n",
    "                                                 min_frequency=params['min_frequency'],\n",
    "                                                 max_length=params['max_length'],\n",
    "                                                 language=params['language'],\n",
    "                                                 vocab_processor=vocab_processor,\n",
    "                                                 shuffle=False)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess = tf.Session()\n",
    "    # Restore metagraph\n",
    "    saver = tf.train.import_meta_graph('{}.meta'.format(os.path.join(FLAGS.run_dir, 'model', FLAGS.checkpoint)))\n",
    "    # Restore weights\n",
    "    saver.restore(sess, os.path.join(FLAGS.run_dir, 'model', FLAGS.checkpoint))\n",
    "\n",
    "    # Get tensors\n",
    "    input_x = graph.get_tensor_by_name('input_x:0')\n",
    "    input_y = graph.get_tensor_by_name('input_y:0')\n",
    "    keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    predictions = graph.get_tensor_by_name('softmax/predictions:0')\n",
    "    accuracy = graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "\n",
    "    # Generate batches\n",
    "    batches = data_helper.batch_iter(data, labels, lengths, FLAGS.batch_size, 1)\n",
    "\n",
    "    num_batches = int(len(data)/FLAGS.batch_size)\n",
    "    all_predictions = []\n",
    "    sum_accuracy = 0\n",
    "\n",
    "    # Test\n",
    "    for batch in batches:\n",
    "        x_test, y_test, x_lengths = batch\n",
    "        if params['clf'] == 'cnn':\n",
    "            feed_dict = {input_x: x_test, input_y: y_test, keep_prob: 1.0}\n",
    "            batch_predictions, batch_accuracy = sess.run([predictions, accuracy], feed_dict)\n",
    "        else:\n",
    "            batch_size = graph.get_tensor_by_name('batch_size:0')\n",
    "            sequence_length = graph.get_tensor_by_name('sequence_length:0')\n",
    "            feed_dict = {input_x: x_test, input_y: y_test, batch_size: FLAGS.batch_size, sequence_length: x_lengths, keep_prob: 1.0}\n",
    "\n",
    "            batch_predictions, batch_accuracy = sess.run([predictions, accuracy], feed_dict)\n",
    "\n",
    "        sum_accuracy += batch_accuracy\n",
    "        all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "    final_accuracy = sum_accuracy / num_batches\n",
    "\n",
    "# Print test accuracy\n",
    "print('Test accuracy: {}'.format(final_accuracy))\n",
    "\n",
    "with open(os.path.join(FLAGS.run_dir, 'predictions.csv'), 'w', encoding='utf-8', newline='') as f:\n",
    "    csvwriter = csv.writer(f)\n",
    "    csvwriter.writerow(['True class', 'Prediction'])\n",
    "    for i in range(len(all_predictions)):\n",
    "        csvwriter.writerow([labels[i], all_predictions[i]])\n",
    "    print('Predictions saved to {}'.format(os.path.join(FLAGS.run_dir, 'predictions.csv')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
