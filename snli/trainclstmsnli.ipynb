{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "wandb: Currently logged in as: bamboo912 (use `wandb login --relogin` to force relogin)\n",
      "D:\\anaconda\\envs\\py36\\lib\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "wandb: wandb version 0.10.28 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">logical-gorge-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/bamboo912/snliclstm\" target=\"_blank\">https://wandb.ai/bamboo912/snliclstm</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/bamboo912/snliclstm/runs/eanj08we\" target=\"_blank\">https://wandb.ai/bamboo912/snliclstm/runs/eanj08we</a><br/>\n",
       "                Run data is saved locally in <code>D:\\code\\Project_jly\\snli\\wandb\\run-20210502_120536-eanj08we</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset ...\n",
      "0\n",
      "1\n",
      "[]\n",
      "[]\n",
      "[2 0 2 ... 0 2 0]\n",
      "[16 15 14 ... 22 47 11]\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\data_helper.py:85: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From D:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "549361\n",
      "[[  1   2   3 ...   0   0   0]\n",
      " [ 14  15   7 ...   0   0   0]\n",
      " [ 12  25  23 ...   0   0   0]\n",
      " ...\n",
      " [  1  32   9 ...   0   0   0]\n",
      " [ 14 963   5 ...   0   0   0]\n",
      " [  1  92  26 ...   0   0   0]]\n",
      "[[   1  184   88 ...    0    0    0]\n",
      " [2592 1250    1 ...    0    0    0]\n",
      " [   1 1181   19 ...    0    0    0]\n",
      " ...\n",
      " [   1   32  528 ...    0    0    0]\n",
      " [   1    5   91 ...    0    0    0]\n",
      " [   1   51   92 ...    0    0    0]]\n",
      "[2 2 2 ... 1 1 2]\n",
      "[24 24 17 ... 12 14 16]\n",
      "<tensorflow.contrib.learn.python.learn.preprocessing.text.VocabularyProcessor object at 0x000001C28F481A90>\n",
      "Dataset has been built successfully.\n",
      "Run time: 42.925567388534546\n",
      "Number of sentences: 549361\n",
      "Vocabulary size: 39606\n",
      "Max document length: 118\n",
      "\n",
      "Parameters:\n",
      "alsologtostderr: False\n",
      "batch_size: 32\n",
      "clf: cnn\n",
      "data_file: D:\\code\\Project_jly\\snli\\data\\train_data.csv\n",
      "decay_rate: 1.0\n",
      "decay_steps: 100000\n",
      "embedding_size: 256\n",
      "evaluate_every_steps: 100\n",
      "f: C:\\Users\\xwangfj\\AppData\\Roaming\\jupyter\\runtime\\kernel-66788bd3-001b-40e4-bc5f-63330561b0ac.json\n",
      "filter_sizes: 3, 4, 5\n",
      "keep_prob: 0.5\n",
      "l2_reg_lambda: 0.001\n",
      "language: en\n",
      "learning_rate: 0.001\n",
      "log_dir: \n",
      "logger_levels: {}\n",
      "logtostderr: False\n",
      "max_length: 118\n",
      "min_frequency: 0\n",
      "num_checkpoint: 10\n",
      "num_classes: 3\n",
      "num_epochs: 10\n",
      "num_filters: 128\n",
      "only_check_args: False\n",
      "op_conversion_fallback_to_while_loop: False\n",
      "pdb: False\n",
      "pdb_post_mortem: False\n",
      "profile_file: None\n",
      "run_with_pdb: False\n",
      "run_with_profiling: False\n",
      "save_every_steps: 1000\n",
      "showprefixforinfo: True\n",
      "stderrthreshold: fatal\n",
      "stop_word_file: None\n",
      "test_random_seed: 301\n",
      "test_randomize_ordering_seed: \n",
      "test_size: 0.1\n",
      "test_srcdir: \n",
      "test_tmpdir: C:\\Users\\xwangfj\\AppData\\Local\\Temp\\absl_testing\n",
      "use_cprofile_for_profiling: True\n",
      "v: -1\n",
      "verbosity: -1\n",
      "vocab_size: 39606\n",
      "xml_output_file: \n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:18: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:27: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:38: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:50: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:62: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From D:\\code\\Project_jly\\snli\\cnn_classifier.py:66: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "Start training ...\n",
      "2021-05-02T12:06:28.533946: step: 1, loss: 9.71647, accuracy: 0.25\n",
      "2021-05-02T12:06:28.701840: step: 2, loss: 5.74727, accuracy: 0.4375\n",
      "2021-05-02T12:06:28.873730: step: 3, loss: 6.37217, accuracy: 0.28125\n",
      "2021-05-02T12:06:29.034628: step: 4, loss: 4.64519, accuracy: 0.34375\n",
      "2021-05-02T12:06:29.201523: step: 5, loss: 5.55129, accuracy: 0.34375\n",
      "2021-05-02T12:06:29.367418: step: 6, loss: 7.53173, accuracy: 0.21875\n",
      "2021-05-02T12:06:29.527316: step: 7, loss: 5.71792, accuracy: 0.21875\n",
      "2021-05-02T12:06:29.697210: step: 8, loss: 4.14522, accuracy: 0.40625\n",
      "2021-05-02T12:06:29.858108: step: 9, loss: 4.15649, accuracy: 0.53125\n",
      "2021-05-02T12:06:30.016007: step: 10, loss: 5.85368, accuracy: 0.40625\n",
      "2021-05-02T12:06:30.183901: step: 11, loss: 5.37237, accuracy: 0.34375\n",
      "2021-05-02T12:06:30.344799: step: 12, loss: 5.74859, accuracy: 0.34375\n",
      "2021-05-02T12:06:30.506697: step: 13, loss: 4.92824, accuracy: 0.34375\n",
      "2021-05-02T12:06:30.663597: step: 14, loss: 5.44657, accuracy: 0.34375\n",
      "2021-05-02T12:06:30.822497: step: 15, loss: 4.65469, accuracy: 0.34375\n",
      "2021-05-02T12:06:30.986393: step: 16, loss: 4.60949, accuracy: 0.3125\n",
      "2021-05-02T12:06:31.150288: step: 17, loss: 3.56862, accuracy: 0.34375\n",
      "2021-05-02T12:06:31.310187: step: 18, loss: 4.23137, accuracy: 0.40625\n",
      "2021-05-02T12:06:31.471086: step: 19, loss: 4.20718, accuracy: 0.40625\n",
      "2021-05-02T12:06:31.637980: step: 20, loss: 5.58785, accuracy: 0.28125\n",
      "2021-05-02T12:06:31.798878: step: 21, loss: 4.16906, accuracy: 0.1875\n",
      "2021-05-02T12:06:31.970769: step: 22, loss: 3.94434, accuracy: 0.28125\n",
      "2021-05-02T12:06:32.140662: step: 23, loss: 5.7267, accuracy: 0.28125\n",
      "2021-05-02T12:06:32.298562: step: 24, loss: 5.36185, accuracy: 0.28125\n",
      "2021-05-02T12:06:32.461458: step: 25, loss: 4.43882, accuracy: 0.375\n",
      "2021-05-02T12:06:32.621357: step: 26, loss: 4.5211, accuracy: 0.28125\n",
      "2021-05-02T12:06:32.780256: step: 27, loss: 4.64857, accuracy: 0.34375\n",
      "2021-05-02T12:06:32.939155: step: 28, loss: 5.46383, accuracy: 0.375\n",
      "2021-05-02T12:06:33.113046: step: 29, loss: 5.0145, accuracy: 0.3125\n",
      "2021-05-02T12:06:33.312434: step: 30, loss: 4.87646, accuracy: 0.375\n",
      "2021-05-02T12:06:33.495319: step: 31, loss: 4.34791, accuracy: 0.40625\n",
      "2021-05-02T12:06:33.687197: step: 32, loss: 5.41875, accuracy: 0.28125\n",
      "2021-05-02T12:06:33.920051: step: 33, loss: 3.27192, accuracy: 0.375\n",
      "2021-05-02T12:06:34.133915: step: 34, loss: 5.19417, accuracy: 0.1875\n",
      "2021-05-02T12:06:34.326793: step: 35, loss: 4.48878, accuracy: 0.28125\n",
      "2021-05-02T12:06:34.512675: step: 36, loss: 2.40034, accuracy: 0.4375\n",
      "2021-05-02T12:06:34.720563: step: 37, loss: 4.46998, accuracy: 0.375\n",
      "2021-05-02T12:06:34.891455: step: 38, loss: 4.67165, accuracy: 0.3125\n",
      "2021-05-02T12:06:35.085364: step: 39, loss: 4.38335, accuracy: 0.5\n",
      "2021-05-02T12:06:35.281240: step: 40, loss: 5.20214, accuracy: 0.375\n",
      "2021-05-02T12:06:35.469122: step: 41, loss: 4.78145, accuracy: 0.34375\n",
      "2021-05-02T12:06:35.668504: step: 42, loss: 2.95684, accuracy: 0.5\n",
      "2021-05-02T12:06:35.833398: step: 43, loss: 3.4005, accuracy: 0.40625\n",
      "2021-05-02T12:06:36.010289: step: 44, loss: 3.30158, accuracy: 0.4375\n",
      "2021-05-02T12:06:36.213158: step: 45, loss: 2.96551, accuracy: 0.375\n",
      "2021-05-02T12:06:36.389594: step: 46, loss: 3.84545, accuracy: 0.375\n",
      "2021-05-02T12:06:36.587428: step: 47, loss: 3.92993, accuracy: 0.3125\n",
      "2021-05-02T12:06:36.752323: step: 48, loss: 3.8543, accuracy: 0.4375\n",
      "2021-05-02T12:06:36.914221: step: 49, loss: 4.55234, accuracy: 0.375\n",
      "2021-05-02T12:06:37.084113: step: 50, loss: 3.18146, accuracy: 0.4375\n",
      "2021-05-02T12:06:37.245011: step: 51, loss: 3.4229, accuracy: 0.53125\n",
      "2021-05-02T12:06:37.419900: step: 52, loss: 4.94616, accuracy: 0.25\n",
      "2021-05-02T12:06:37.579799: step: 53, loss: 4.60855, accuracy: 0.375\n",
      "2021-05-02T12:06:37.747693: step: 54, loss: 4.12115, accuracy: 0.375\n",
      "2021-05-02T12:06:37.911589: step: 55, loss: 3.2904, accuracy: 0.40625\n",
      "2021-05-02T12:06:38.078483: step: 56, loss: 4.5303, accuracy: 0.34375\n",
      "2021-05-02T12:06:38.240381: step: 57, loss: 3.84905, accuracy: 0.3125\n",
      "2021-05-02T12:06:38.404277: step: 58, loss: 7.16268, accuracy: 0.21875\n",
      "2021-05-02T12:06:38.562178: step: 59, loss: 4.5019, accuracy: 0.34375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-02T12:06:38.732069: step: 60, loss: 3.92238, accuracy: 0.3125\n",
      "2021-05-02T12:06:38.892967: step: 61, loss: 3.10083, accuracy: 0.53125\n",
      "2021-05-02T12:06:39.057864: step: 62, loss: 5.50546, accuracy: 0.28125\n",
      "2021-05-02T12:06:39.223758: step: 63, loss: 4.07832, accuracy: 0.3125\n",
      "2021-05-02T12:06:39.394651: step: 64, loss: 4.83514, accuracy: 0.46875\n",
      "2021-05-02T12:06:39.558546: step: 65, loss: 3.70266, accuracy: 0.40625\n",
      "2021-05-02T12:06:39.725440: step: 66, loss: 3.42017, accuracy: 0.40625\n",
      "2021-05-02T12:06:39.881341: step: 67, loss: 4.54962, accuracy: 0.3125\n",
      "2021-05-02T12:06:40.044238: step: 68, loss: 2.92068, accuracy: 0.40625\n",
      "2021-05-02T12:06:40.203138: step: 69, loss: 3.32219, accuracy: 0.40625\n",
      "2021-05-02T12:06:40.374030: step: 70, loss: 4.53483, accuracy: 0.40625\n",
      "2021-05-02T12:06:40.569906: step: 71, loss: 5.13761, accuracy: 0.3125\n",
      "2021-05-02T12:06:40.748792: step: 72, loss: 3.17266, accuracy: 0.40625\n",
      "2021-05-02T12:06:40.917685: step: 73, loss: 2.45354, accuracy: 0.46875\n",
      "2021-05-02T12:06:41.087578: step: 74, loss: 4.40149, accuracy: 0.375\n",
      "2021-05-02T12:06:41.247477: step: 75, loss: 3.59807, accuracy: 0.4375\n",
      "2021-05-02T12:06:41.412372: step: 76, loss: 3.38323, accuracy: 0.40625\n",
      "2021-05-02T12:06:41.571271: step: 77, loss: 4.81919, accuracy: 0.21875\n",
      "2021-05-02T12:06:41.739165: step: 78, loss: 5.80604, accuracy: 0.21875\n",
      "2021-05-02T12:06:41.900063: step: 79, loss: 5.41187, accuracy: 0.3125\n",
      "2021-05-02T12:06:42.069956: step: 80, loss: 4.91223, accuracy: 0.25\n",
      "2021-05-02T12:06:42.228855: step: 81, loss: 3.67077, accuracy: 0.3125\n",
      "2021-05-02T12:06:42.392751: step: 82, loss: 3.40914, accuracy: 0.34375\n",
      "2021-05-02T12:06:42.552650: step: 83, loss: 4.07475, accuracy: 0.3125\n",
      "2021-05-02T12:06:42.719544: step: 84, loss: 3.84566, accuracy: 0.3125\n",
      "2021-05-02T12:06:42.876445: step: 85, loss: 5.32026, accuracy: 0.28125\n",
      "2021-05-02T12:06:43.034344: step: 86, loss: 3.0729, accuracy: 0.375\n",
      "2021-05-02T12:06:43.197242: step: 87, loss: 3.63537, accuracy: 0.3125\n",
      "2021-05-02T12:06:43.359139: step: 88, loss: 3.32893, accuracy: 0.375\n",
      "2021-05-02T12:06:43.523036: step: 89, loss: 3.94407, accuracy: 0.4375\n",
      "2021-05-02T12:06:43.689930: step: 90, loss: 6.99415, accuracy: 0.375\n",
      "2021-05-02T12:06:43.867817: step: 91, loss: 2.35748, accuracy: 0.53125\n",
      "2021-05-02T12:06:44.038708: step: 92, loss: 4.41129, accuracy: 0.3125\n",
      "2021-05-02T12:06:44.200606: step: 93, loss: 3.51053, accuracy: 0.40625\n",
      "2021-05-02T12:06:44.363503: step: 94, loss: 3.55692, accuracy: 0.375\n",
      "2021-05-02T12:06:44.528398: step: 95, loss: 3.18306, accuracy: 0.28125\n",
      "2021-05-02T12:06:44.693294: step: 96, loss: 4.18706, accuracy: 0.4375\n",
      "2021-05-02T12:06:44.855191: step: 97, loss: 2.72808, accuracy: 0.53125\n",
      "2021-05-02T12:06:45.015090: step: 98, loss: 3.95667, accuracy: 0.34375\n",
      "2021-05-02T12:06:45.189979: step: 99, loss: 2.44818, accuracy: 0.46875\n",
      "2021-05-02T12:06:45.354875: step: 100, loss: 3.68087, accuracy: 0.4375\n",
      "\n",
      "Validation\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import wandb\n",
    "wandb.init(project=\"snliclstm\")\n",
    "\n",
    "import data_helper\n",
    "from rnn_classifier import rnn_clf\n",
    "from cnn_classifier import cnn_clf\n",
    "from clstm_classifier import clstm_clf\n",
    "\n",
    "###### 这段作用是啥\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as e:\n",
    "    error = \"Please install scikit-learn.\"\n",
    "    print(str(e) + ': ' + error)\n",
    "    sys.exit()\n",
    "\n",
    "# Show warnings and errors only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Parameters\n",
    "# =============================================================================\n",
    "\n",
    "# Model choices\n",
    "tf.flags.DEFINE_string('clf', 'cnn', \"Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm]\")\n",
    "\n",
    "# Data parameters\n",
    "tf.flags.DEFINE_string('data_file', 'D:\\\\code\\\\Project_jly\\\\snli\\\\data\\\\train_data.csv', 'Data file path')\n",
    "tf.flags.DEFINE_string('stop_word_file', None, 'Stop word file path')\n",
    "tf.flags.DEFINE_string('language', 'en', \"Language of the data file. You have two choices: [ch, en]\")\n",
    "tf.flags.DEFINE_integer('min_frequency', 0, 'Minimal word frequency')\n",
    "tf.flags.DEFINE_integer('num_classes', 3, 'Number of classes')\n",
    "tf.flags.DEFINE_integer('max_length', 0, 'Max document length')\n",
    "tf.flags.DEFINE_integer('vocab_size', 0, 'Vocabulary size')\n",
    "tf.flags.DEFINE_float('test_size', 0.1, 'Cross validation test size')  ############### add vy myself change 0.1 to 0\n",
    "\n",
    "# Model hyperparameters\n",
    "tf.flags.DEFINE_integer('embedding_size', 256, 'Word embedding size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_string('filter_sizes', '3, 4, 5', 'CNN filter sizes. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('num_filters', 128, 'Number of filters per filter size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('hidden_size', 128, 'Number of hidden units in the LSTM cell. For LSTM, Bi-LSTM')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'Number of the LSTM cells. For LSTM, Bi-LSTM, C-LSTM')\n",
    "tf.flags.DEFINE_float('keep_prob', 0.5, 'Dropout keep probability')  # All\n",
    "tf.flags.DEFINE_float('learning_rate', 1e-3, 'Learning rate')  # All\n",
    "tf.flags.DEFINE_float('l2_reg_lambda', 0.001, 'L2 regularization lambda')  # All\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer('batch_size', 32, 'Batch size')\n",
    "tf.flags.DEFINE_integer('num_epochs', 10, 'Number of epochs')\n",
    "tf.flags.DEFINE_float('decay_rate', 1, 'Learning rate decay rate. Range: (0, 1]')  # Learning rate decay\n",
    "tf.flags.DEFINE_integer('decay_steps', 100000, 'Learning rate decay steps')  # Learning rate decay\n",
    "tf.flags.DEFINE_integer('evaluate_every_steps', 100, 'Evaluate the model on validation set after this many steps')\n",
    "tf.flags.DEFINE_integer('save_every_steps', 1000, 'Save the model after this many steps')\n",
    "tf.flags.DEFINE_integer('num_checkpoint', 10, 'Number of models to store')\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS    #### 奥，直接用FLAGS 代替tf.app.flags，封装好的结构体\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel') ##add by me 不懂加这行的原因\n",
    "\n",
    "if FLAGS.clf == 'lstm':\n",
    "    FLAGS.embedding_size = FLAGS.hidden_size\n",
    "elif FLAGS.clf == 'clstm':\n",
    "    FLAGS.hidden_size = len(FLAGS.filter_sizes.split(\",\")) * FLAGS.num_filters\n",
    "\n",
    "# Output files directory \n",
    "timestamp = str(int(time.time()))\n",
    "outdir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Load and save data\n",
    "# =============================================================================\n",
    "###### 仔细看一下data_helper 文件\n",
    "data, labels, lengths, vocab_processor = data_helper.load_data(file_path=FLAGS.data_file,\n",
    "                                                               sw_path=FLAGS.stop_word_file,\n",
    "                                                               min_frequency=FLAGS.min_frequency,\n",
    "                                                               max_length=FLAGS.max_length,\n",
    "                                                               language=FLAGS.language,\n",
    "                                                               shuffle=True)\n",
    "# add by myself\n",
    "\n",
    "#\n",
    "# Save vocabulary processor   这些参数vocab_size，max_length也和data_helper有关\n",
    "vocab_processor.save(os.path.join(outdir, 'vocab'))\n",
    "\n",
    "FLAGS.vocab_size = len(vocab_processor.vocabulary_._mapping)\n",
    "\n",
    "FLAGS.max_length = vocab_processor.max_document_length\n",
    "\n",
    "params = FLAGS.flag_values_dict()   #####这个命令把前面FLAGS定义的参数归到了paras中\n",
    "# 注：FLAGS._parse_flags() 改用FLAGS.flag_values_dict()将其解析成字典存储到FLAGS.__flags中\n",
    "# Print parameters\n",
    "model = params['clf']\n",
    "if model == 'cnn':\n",
    "    del params['hidden_size']\n",
    "    del params['num_layers']\n",
    "elif model == 'lstm' or model == 'blstm':\n",
    "    del params['num_filters']\n",
    "    del params['filter_sizes']\n",
    "    params['embedding_size'] = params['hidden_size']\n",
    "elif model == 'clstm':\n",
    "    params['hidden_size'] = len(list(map(int, params['filter_sizes'].split(\",\")))) * params['num_filters']\n",
    "\n",
    "params_dict = sorted(params.items(), key=lambda x: x[0]) # sorted() 函数对所有可迭代的对象进行排序操作。按照 x[0]这个元素排序\n",
    "print('Parameters:')\n",
    "for item in params_dict:\n",
    "    print('{}: {}'.format(item[0], item[1]))\n",
    "print('')\n",
    "\n",
    "# Save parameters to file\n",
    "params_file = open(os.path.join(outdir, 'params.pkl'), 'wb')\n",
    "pkl.dump(params, params_file, True)\n",
    "params_file.close()\n",
    "\n",
    "\n",
    "# Simple Cross validation   train_test_split是库里面的函数，data是前面用data_helper.load data加载进来的,用来划分训练集和测试集的\n",
    "x_train, x_valid, y_train, y_valid, train_lengths, valid_lengths = train_test_split(data,\n",
    "                                                                                    labels,\n",
    "                                                                                    lengths,\n",
    "                                                                                    test_size=FLAGS.test_size,\n",
    "                                                                                    random_state=22)\n",
    "# Batch iterator\n",
    "train_data = data_helper.batch_iter(x_train, y_train, train_lengths, FLAGS.batch_size, FLAGS.num_epochs)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        if FLAGS.clf == 'cnn':\n",
    "            classifier = cnn_clf(FLAGS)   ####不用管数据的吗，只输入参数的吗\n",
    "        elif FLAGS.clf == 'lstm' or FLAGS.clf == 'blstm':\n",
    "            classifier = rnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'clstm':\n",
    "            classifier = clstm_clf(FLAGS)\n",
    "        else:\n",
    "            raise ValueError('clf should be one of [cnn, lstm, blstm, clstm]')  ###确定模型\n",
    "\n",
    "        # Train procedure\n",
    "        # 我们通过tf.Variable构造一个variable添加进图中，Variable()构造函数需要变量的初始值(是一个任意类型、任意形状的tensor)，\n",
    "        # 这个初始值指定variable的类型和形状。通过Variable()构造函数后，此variable的类型和形状固定不能修改了，但值可以用assign方法修改。\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)   ###定义给学习率递减用\n",
    "        # Learning rate decay\n",
    "        starter_learning_rate = FLAGS.learning_rate   ###定义给学习率递减用\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate,\n",
    "                                                   global_step,\n",
    "                                                   FLAGS.decay_steps,\n",
    "                                                   FLAGS.decay_rate,\n",
    "                                                   staircase=True)  ###学习率递减\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate) ###选择参数优化方法\n",
    "        grads_and_vars = optimizer.compute_gradients(classifier.cost)  ###计算cost函数的梯度\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step) ###结合优化方法和梯度\n",
    "\n",
    "        # Summaries   summary这是将结果这个方法是添加变量到直方图中，在prompt输入tensorboard --logdir=文件夹绝对路径\\ 即可得到tensorboard 的网址然后得到图形\n",
    "        # 而在训练过程中，主要用到了tf.summary()的各类方法，能够保存训练过程以及参数分布图并在tensorboard显示。\n",
    "        # tf.summary有诸多函数：1、tf.summary.scalar用来显示标量信息\n",
    "        # 2、tf.summary.histogram 用来显示直方图信息\n",
    "        loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "        accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "        # Train summary  写入了文件\n",
    "        # merge_all 可以将所有summary全部保存到磁盘，以便tensorboard显示。如果没有特殊要求，一般用这一句就可一显示训练时的各种信息了\n",
    "        # 8、tf.summary.FileWriter 指定一个文件用来保存图。\n",
    "        train_summary_op = tf.summary.merge_all()\n",
    "        train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summary 写入了文件\n",
    "        valid_summary_op = tf.summary.merge_all()\n",
    "        valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "        valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "        # 后面用了saver一次，但是这是干啥的不清楚\n",
    "        # 将Saver类添加ops 从而在checkpointes里save和restore变量 。它还提供了运行这些操作的便捷方法。\n",
    "        # Checkpoints是专有格式的二进制文件，它将变量名称映射到张量值。测试Checkpoints内容的最佳方式是使用Saver来加载它\n",
    "        # Savers可以使用提供的计数器自动为Checkpoint文件名编号，这使您可以在训练模型时在不同的步骤中保留多个Checkpoints。\n",
    "        # 例如，您可以使用训练步骤编号对Checkpoint文件名进行编号。为避免填满磁盘，储存器会自动管理Checkpoint文件。例如，他们只能保留N个最新文件，或每N小时训练一个Checkpoint。\n",
    "        # max_to_keep表示要保留的最近文件的最大数量。创建新文件时，将删除旧文件。如果为None或0，则不会从文件系统中删除任何Checkpoint，但只有最后一个Checkpoint保留在checkpoint文件中。\n",
    "        # 默认为5（即保留最近的5个Checkpoint文件。）\n",
    "        # 这个命令只是定义一下，后面用\n",
    "        saver = tf.train.Saver(max_to_keep=FLAGS.num_checkpoint)\n",
    "\n",
    "        # 这是干啥的不清楚\n",
    "        # 我们在编写代码的时候，总是要先定义好整个图，然后才调用sess.run()。那么调用sess.run()的时候，程序是否执行了整个图\n",
    "        # 添加节点用于初始化全局变量(GraphKeys.GLOBAL_VARIABLES)。返回一个初始化所有全局变量的操作（Op）\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        # 定义训练函数\n",
    "        def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "            input_x, input_y, sequence_length = input_data\n",
    "\n",
    "            # 当我们构建完图（可能是我们pre_process后生成的图片？NoNoNo，它只是指tensorflow框架的一种设计理念——计算流图）后，需要在一个会话中启动图，启动的第一步是创建一个Session对象。\n",
    "            # 为了取回（Fetch）操作的输出内容, 可以在使用 Session 对象的 run()调用执行图时，传入一些 tensor, 这些 tensor 会帮助你取回结果。\n",
    "\n",
    "\n",
    "            # 参数已在cnn，clstm中训练好\n",
    "            fetches = {'step': global_step,\n",
    "                       'cost': classifier.cost,\n",
    "                       'accuracy': classifier.accuracy,\n",
    "                       'learning_rate': learning_rate}\n",
    "\n",
    "            # cnn，clstm模型用的input_x\n",
    "            feed_dict = {classifier.input_x: input_x,\n",
    "                         classifier.input_y: input_y}\n",
    "\n",
    "            # fetches, feed_dict是什么封装的结构，干什么的，这里咋这样训练，咋还用到了accuracy和summaries\n",
    "            if FLAGS.clf != 'cnn':\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "\n",
    "            if is_training:\n",
    "                fetches['train_op'] = train_op\n",
    "                fetches['summaries'] = train_summary_op\n",
    "                feed_dict[classifier.keep_prob] = FLAGS.keep_prob\n",
    "            else:\n",
    "                fetches['summaries'] = valid_summary_op\n",
    "                feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "            vars = sess.run(fetches, feed_dict)\n",
    "            # 返回对象object的属性和属性值的字典对象，如果没有参数，就打印当前调用位置的属性和属性值 类似 locals()。\n",
    "            step = vars['step']\n",
    "            cost = vars['cost']\n",
    "            accuracy = vars['accuracy']\n",
    "            summaries = vars['summaries']\n",
    "\n",
    "            # Write summaries to file\n",
    "            if is_training:\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "            else:\n",
    "                valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step: {}, loss: {:g}, accuracy: {:g}\".format(time_str, step, cost, accuracy))\n",
    "\n",
    "            return accuracy\n",
    "\n",
    "\n",
    "        print('Start training ...')\n",
    "\n",
    "        for train_input in train_data:\n",
    "            run_step(train_input, is_training=True)\n",
    "            current_step = tf.train.global_step(sess, global_step)  #这是个提取当前步骤的命令\n",
    "            # global_step是指图中看到的批次数量。每次提供一个批处理时，权重都会按照最小化损失的方向更新。\n",
    "            # global_step只是跟踪到目前为止看到的批数。当在minimum()参数列表中传递该变量时，该变量将增加1。查看optimizer. minimum()，你可以使用tf.train.global_step()获得global_step值。\n",
    "\n",
    "            ## 多少步训练和交叉验证\n",
    "            if current_step % FLAGS.evaluate_every_steps == 0:\n",
    "                print('\\nValidation')\n",
    "                run_step((x_valid, y_valid, valid_lengths), is_training=False)\n",
    "                print('')\n",
    "            ## 多少步保存\n",
    "            if current_step % FLAGS.save_every_steps == 0:\n",
    "                save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "\n",
    "        print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
